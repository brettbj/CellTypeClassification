{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "directory = './Images/'\n",
    "# input image dimensions\n",
    "# img_rows, img_cols = 1024, 1024\n",
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "patch_count = 10\n",
    "\n",
    "def load(seed=123, set=0):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for i in [2, 4]:\n",
    "        cur_direct = directory + 'Cluster ' + str(i) + '/'\n",
    "        j = 0\n",
    "        label = i/2-1\n",
    "        print(label)\n",
    "        for jpg in os.listdir(cur_direct):\n",
    "            j += 1\n",
    "            \n",
    "            x = ndimage.imread(cur_direct + jpg)\n",
    "            patches = extract_patches_2d(x, (img_rows, img_cols), patch_count)\n",
    "    \n",
    "            if j >= (set*10) and j < (set*10+10):\n",
    "                X_test.extend(patches)  \n",
    "                y_test.extend([label] * patch_count)\n",
    "            else:\n",
    "                X.extend(patches)\n",
    "                y.extend([label] * patch_count)\n",
    "\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y = np.asarray(y)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    X = X.reshape(X.shape[0], 3, img_rows, img_cols)\n",
    "    X, y = shuffle(X, y, random_state=seed)  # shuffle train data\n",
    "    \n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "    \n",
    "    X = X.astype('float32')\n",
    "    y = y.astype('float32')\n",
    "    \n",
    "    X_test = X_test.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "    return (X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "(1980, 128, 128, 3)\n",
      "(1980,)\n",
      "(1980, 3, 128, 128)\n",
      "(1980,)\n",
      "('X_train shape:', (1980, 3, 128, 128))\n",
      "(1980, 'train samples')\n",
      "(180, 'test samples')\n",
      "((1980,), (180,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1980 samples, validate on 180 samples\n",
      "Epoch 1/10\n",
      "1980/1980 [==============================] - 24s - loss: 1.5077 - acc: 0.5040 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.6779 - acc: 0.6020 - val_loss: 0.6505 - val_acc: 0.8333\n",
      "Epoch 3/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.4690 - acc: 0.8672 - val_loss: 0.4760 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.3359 - acc: 0.8879 - val_loss: 0.6946 - val_acc: 0.6833\n",
      "Epoch 5/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.2881 - acc: 0.8904 - val_loss: 0.5174 - val_acc: 0.7444\n",
      "Epoch 6/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.1681 - acc: 0.9419 - val_loss: 0.4429 - val_acc: 0.7667\n",
      "Epoch 7/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.1283 - acc: 0.9596 - val_loss: 0.4086 - val_acc: 0.8056\n",
      "Epoch 8/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.0938 - acc: 0.9692 - val_loss: 0.4289 - val_acc: 0.8167\n",
      "Epoch 9/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.0759 - acc: 0.9737 - val_loss: 0.4859 - val_acc: 0.8111\n",
      "Epoch 10/10\n",
      "1980/1980 [==============================] - 19s - loss: 0.0642 - acc: 0.9753 - val_loss: 0.4810 - val_acc: 0.8111\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.48095114823016855)\n",
      "('Test accuracy:', 0.81111111376020639)\n",
      "[0.81111111376020639]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 21s - loss: 1.6462 - acc: 0.4949 - val_loss: 0.6925 - val_acc: 0.5350\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6839 - acc: 0.5923 - val_loss: 0.6428 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.5535 - acc: 0.7791 - val_loss: 0.5534 - val_acc: 0.7200\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.4137 - acc: 0.8347 - val_loss: 0.3584 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3123 - acc: 0.8832 - val_loss: 0.2636 - val_acc: 0.9050\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.2031 - acc: 0.9311 - val_loss: 0.1847 - val_acc: 0.9500\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1369 - acc: 0.9505 - val_loss: 0.0828 - val_acc: 0.9800\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0974 - acc: 0.9663 - val_loss: 0.0555 - val_acc: 0.9850\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1127 - acc: 0.9577 - val_loss: 0.0849 - val_acc: 0.9600\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.0956 - acc: 0.9607 - val_loss: 0.0734 - val_acc: 0.9500\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.073366037770174447)\n",
      "('Test accuracy:', 0.94999999999999996)\n",
      "[0.81111111376020639, 0.94999999999999996]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 2.5181 - acc: 0.4949 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6870 - acc: 0.4954 - val_loss: 0.6692 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6588 - acc: 0.4974 - val_loss: 0.6052 - val_acc: 0.7800\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.5742 - acc: 0.6214 - val_loss: 0.5972 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.4773 - acc: 0.8337 - val_loss: 0.4564 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3995 - acc: 0.8949 - val_loss: 0.1961 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3026 - acc: 0.8832 - val_loss: 0.0515 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2438 - acc: 0.9184 - val_loss: 0.1963 - val_acc: 0.9500\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2028 - acc: 0.9245 - val_loss: 0.1760 - val_acc: 0.9500\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1567 - acc: 0.9413 - val_loss: 0.1966 - val_acc: 0.9500\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.1965920040011406)\n",
      "('Test accuracy:', 0.94999999999999996)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.1800 - acc: 0.5077 - val_loss: 0.6629 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.6336 - acc: 0.5010 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.4986 - acc: 0.7036 - val_loss: 0.2999 - val_acc: 0.9800\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.2914 - acc: 0.9097 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.2087 - acc: 0.9184 - val_loss: 0.0367 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.1529 - acc: 0.9459 - val_loss: 0.0319 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.1228 - acc: 0.9490 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.1186 - acc: 0.9577 - val_loss: 0.0328 - val_acc: 0.9950\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.0922 - acc: 0.9628 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.0896 - acc: 0.9643 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.009286663911479991)\n",
      "('Test accuracy:', 1.0)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.6902 - acc: 0.4852 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6930 - acc: 0.5046 - val_loss: 0.6910 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6797 - acc: 0.5408 - val_loss: 0.6408 - val_acc: 0.5400\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.5603 - acc: 0.7413 - val_loss: 0.4152 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3365 - acc: 0.9393 - val_loss: 0.0961 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1845 - acc: 0.9255 - val_loss: 0.0546 - val_acc: 0.9650\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1287 - acc: 0.9520 - val_loss: 0.0944 - val_acc: 0.9550\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0856 - acc: 0.9689 - val_loss: 0.0358 - val_acc: 0.9950\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0703 - acc: 0.9730 - val_loss: 0.0753 - val_acc: 0.9550\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0635 - acc: 0.9750 - val_loss: 0.0992 - val_acc: 0.9500\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.099197257179766893)\n",
      "('Test accuracy:', 0.94999999999999996)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 2.2960 - acc: 0.4765 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6950 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.6050\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6915 - acc: 0.4934 - val_loss: 0.6910 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6874 - acc: 0.5173 - val_loss: 0.6836 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6773 - acc: 0.5219 - val_loss: 0.6683 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.6554 - acc: 0.5520 - val_loss: 0.5981 - val_acc: 0.9450\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.5084 - acc: 0.7480 - val_loss: 0.2914 - val_acc: 0.9500\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3758 - acc: 0.7847 - val_loss: 0.2135 - val_acc: 0.9500\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3212 - acc: 0.7980 - val_loss: 0.2288 - val_acc: 0.9500\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.2348 - acc: 0.8352 - val_loss: 0.2036 - val_acc: 0.9500\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.20364596180617808)\n",
      "('Test accuracy:', 0.94999999999999996)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996, 0.94999999999999996]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.1984 - acc: 0.4990 - val_loss: 0.6844 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6560 - acc: 0.4949 - val_loss: 0.6441 - val_acc: 0.6500\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.5788 - acc: 0.5551 - val_loss: 0.5895 - val_acc: 0.7800\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.4600 - acc: 0.8005 - val_loss: 0.4925 - val_acc: 0.8650\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.3927 - acc: 0.8852 - val_loss: 0.4994 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3482 - acc: 0.9214 - val_loss: 0.3958 - val_acc: 0.8900\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.3057 - acc: 0.9367 - val_loss: 0.3629 - val_acc: 0.8850\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2711 - acc: 0.9474 - val_loss: 0.3051 - val_acc: 0.8950\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2355 - acc: 0.9561 - val_loss: 0.2493 - val_acc: 0.9150\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2147 - acc: 0.9526 - val_loss: 0.3143 - val_acc: 0.8550\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.31431475266814229)\n",
      "('Test accuracy:', 0.85499999999999998)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996, 0.94999999999999996, 0.85499999999999998]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.8760 - acc: 0.4954 - val_loss: 0.6937 - val_acc: 0.3750\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6791 - acc: 0.5204 - val_loss: 0.6794 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6266 - acc: 0.5908 - val_loss: 0.7087 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.3944 - acc: 0.8653 - val_loss: 0.8995 - val_acc: 0.6050\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2486 - acc: 0.8964 - val_loss: 0.2766 - val_acc: 0.8950\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1701 - acc: 0.9347 - val_loss: 0.3895 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 19s - loss: 0.1179 - acc: 0.9571 - val_loss: 0.4704 - val_acc: 0.7850\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0891 - acc: 0.9633 - val_loss: 0.2966 - val_acc: 0.9400\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0771 - acc: 0.9648 - val_loss: 0.4084 - val_acc: 0.8950\n",
      "Epoch 00008: early stopping\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.40842461817897857)\n",
      "('Test accuracy:', 0.89500000000000002)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996, 0.94999999999999996, 0.85499999999999998, 0.89500000000000002]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.3610 - acc: 0.4867 - val_loss: 0.6690 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6564 - acc: 0.6867 - val_loss: 0.6363 - val_acc: 0.5850\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.5345 - acc: 0.7378 - val_loss: 0.2915 - val_acc: 0.9700\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2810 - acc: 0.9005 - val_loss: 0.0353 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2051 - acc: 0.9224 - val_loss: 0.0311 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1521 - acc: 0.9418 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1328 - acc: 0.9469 - val_loss: 0.0515 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1448 - acc: 0.9444 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1034 - acc: 0.9577 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0865 - acc: 0.9602 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.021943014343851246)\n",
      "('Test accuracy:', 1.0)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996, 0.94999999999999996, 0.85499999999999998, 0.89500000000000002, 1.0]\n",
      "0\n",
      "1\n",
      "(1960, 128, 128, 3)\n",
      "(1960,)\n",
      "(1960, 3, 128, 128)\n",
      "(1960,)\n",
      "('X_train shape:', (1960, 3, 128, 128))\n",
      "(1960, 'train samples')\n",
      "(200, 'test samples')\n",
      "((1960,), (200,))\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "Train on 1960 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1960/1960 [==============================] - 19s - loss: 1.2542 - acc: 0.4918 - val_loss: 0.6710 - val_acc: 0.6550\n",
      "Epoch 2/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.6473 - acc: 0.5005 - val_loss: 0.6207 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.4783 - acc: 0.7444 - val_loss: 0.4664 - val_acc: 0.8550\n",
      "Epoch 4/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.2882 - acc: 0.8974 - val_loss: 0.8293 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1698 - acc: 0.9357 - val_loss: 0.2368 - val_acc: 0.9250\n",
      "Epoch 6/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.1185 - acc: 0.9587 - val_loss: 0.2416 - val_acc: 0.8800\n",
      "Epoch 7/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0861 - acc: 0.9694 - val_loss: 0.3911 - val_acc: 0.8500\n",
      "Epoch 8/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0671 - acc: 0.9791 - val_loss: 0.7827 - val_acc: 0.7950\n",
      "Epoch 9/10\n",
      "1960/1960 [==============================] - 18s - loss: 0.0705 - acc: 0.9755 - val_loss: 0.3359 - val_acc: 0.8500\n",
      "Epoch 00008: early stopping\n",
      "('Metrics', ['loss', 'acc'])\n",
      "('Test score:', 0.33593461275100706)\n",
      "('Test accuracy:', 0.84999999999999998)\n",
      "[0.81111111376020639, 0.94999999999999996, 0.94999999999999996, 1.0, 0.94999999999999996, 0.94999999999999996, 0.85499999999999998, 0.89500000000000002, 1.0, 0.84999999999999998]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "\n",
    "nb_classes = 2\n",
    "accuracy = []\n",
    "nb_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "score_list = []\n",
    "for i in range(10):\n",
    "    X, y, X_test, y_test = load(set=i)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_train = X\n",
    "    X_test = X_test\n",
    "    y_train = y\n",
    "    y_test = y_test\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "    print(y_train.shape, y_test.shape)\n",
    "\n",
    "    print(np.unique(y_train))\n",
    "    print(np.unique(y_test))\n",
    "    # convert class vectors to binary class matrices\n",
    "    # Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    # Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    Y_train = y_train\n",
    "    Y_test = y_test\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                            input_shape=(3, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    opt = Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    model.fit(X_train,Y_train, batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              callbacks=[early_stop])\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    score_list.append(score[1])\n",
    "    print('Metrics', model.metrics_names)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    accuracy.append(score[1])\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f63c80134d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEMCAYAAADd+e2FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63c814e3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "ax = sns.boxplot(score_list)\n",
    "ax = sns.swarmplot(score_list, color=\".25\")\n",
    "sns.plt.title('Cross Validation Balanced Accuracy')\n",
    "#plt.boxplot(score_list, 1, 'gD', ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
